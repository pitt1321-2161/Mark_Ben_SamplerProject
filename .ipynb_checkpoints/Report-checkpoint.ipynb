{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Inspired by the wave analyses and manipulations that homework 9 required, as well as by our own experiences as musicians, we chose to create what we are calling an autosampler. In a musical context, sampling is the act of selecting and isolating small parts (known as samples) of a recording, and using those samples as part of a new piece of music. Sampling has always been common in the world of hip hop, where drum beat samples are looped and used as a backing track. More recently, some producers of electronic dance music have risen to prominence whose music consists almost entirely of short samples, mixed together and played as their own instrument. In either case, the producer needed to manually create his own samples. Using modern-day software, this is done by simply selecting a starting and an ending point on a recording's sound wave, and taking the part of the wave that lies between those two points. While this process sounds simple enough, it becomes quite tedious when you have large numbers of recordings to analyze, listen to for potential samples, and cut samples from. Our goal in this project is to automatically cut samples from user-provided wave files, and arrange those samples to produce output in the tune of a user-provided melody. For the examples presented in this paper, we have used the melody of Beethoven's well known piece Fur Elise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE DESCRIPTION\n",
    "\n",
    "The meat of this project is the functionality of chopping up an input wave into samples, and then properly categorizing and storing the resulting pieces. In our initial, naive approach, we decided to chop the waves such that one sample would have the length of an eighth note, given a predefined tempo in beats per minute. We arbitrarily chose a tempo of 148 beats per minute. This translated to approximately 2.467 beats per second, or 0.2027 seconds per note since the eighth note is traditionally half a beat. Using a standard sampling rate of 44.1 kHz, this meant that each note would be 8939 data points long.\n",
    "\n",
    "With this information, the next step is to iterate over each input wave provided by the user. Using the functions from waveIO.py, each file is read in and unpacked.The wave is then split, as mentioned above, into pieces 8939 data points long. This list of points serves as the samples that will be analyzed, sorted, and built into a final output wave.\n",
    "\n",
    "This list of samples is then iterated over for sorting and storage. Samples shorter than 8939 samples long are discarded, which prevents shortened samples such as those that can occur at the end of a wave from being sorted with the samples of uniform length. The Fourier Transform of the sample is taken using Numpy's fft module, and then divided by the length of the sample in data points to normalize its amplitudes. Numpy'sfftfreq function is used to create the frequency bins for the sample, and the sample's strongest frequency is found.\n",
    "\n",
    "This is where the amplitude normalization becomes important. The amplitude of the strongest frequency is compared to a predefined threshold value. If the amplitude is lower than this threshold, then no frequencies are extracted from the sample. If the amplitude is greater than the threshold, the sample's data is added to a large list of samples, and its index within that list is noted. The sample proceeds with sorting. In that case, the frequency is compared with a dictionary containing baseline frequencies for the twelve semitones in a musical octave. Since the A4 (this notation stands for the note A in the fourth octave, with A being the tenth not in the octave) note with an ideal frequency of 440.0 Hz is typically used as a standard, I used the frequencies of all the fourth octave notes to populate this dictionary.\n",
    "\n",
    "If the sample's frequency is lower than the lowest frequency in the dictionary (with about 1% tolerance), then it must be part of a lower octave. Likewise, if it is higher than the highest frequency in the dictionary (with the same tolerance), it must be part of a higher octave. In the former case, I divide a multiplier value (initially set to 1) by 2 and compare the sample frequency to the highest and lowest frequencies in the dictionary, multiplied by the multiplier value. If the frequency is still too low, I again halve the multiplier, continuing this process until the sample frequency falls between the highest and lowest dictionary frequencies times the multiplier. Alternatively, in the case that the sample frequency was too high, I perform the same process, but double the multiplier instead of halving it on each unsuccessful comparison.\n",
    "\n",
    "Having ascertained the octave of the sample frequency, and with specific values of the multiplier corresponding to specific octaves, the sample can be properly stored. The dictionary of note frequencies is iterated over, with each note's frequency value multiplied by the multiplier from earlier. If the sample frequency is within 1% of this multiplied dictionary value, its index in the larger sample list is stored in a list within a dictionary of the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notes_db = {\n",
    "\"A\": [[],[],[],[],[],[]],\n",
    "\"A#\": [[],[],[],[],[],[]],\n",
    "\"B\": [[],[],[],[],[],[]],\n",
    "\"C\": [[],[],[],[],[],[]],\n",
    "\"C#\": [[],[],[],[],[],[]],\n",
    "\"D\": [[],[],[],[],[],[]],\n",
    "\"D#\": [[],[],[],[],[],[]],\n",
    "\"E\": [[],[],[],[],[],[]],\n",
    "\"F\": [[],[],[],[],[],[]],\n",
    "\"F#\": [[],[],[],[],[],[]],\n",
    "\"G\": [[],[],[],[],[],[]],\n",
    "\"G#\":[[],[],[],[],[],[]]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where each note has a list of sub-lists, and each sub-list holds the index values of samples that contain the corresponding note in a specific octave.\n",
    "\n",
    "If the code were to stop at this point, it could deal perfectly fine with simple input waves of single notes. Chords, however, would pose a problem, as only one frequency is pulled from each sample. To solve this problem, after the sample is sorted into one of the bins above, the strongest frequency is deleted from the Fourier Transform data, as is the frequency bin that corresponded to that frequency. The octave multiplier is reset to 1, and the next strongest frequency is pulled from the Fourier data. Its amplitude is once again compared to the threshold, and the whole process repeats itself, selecting, sorting, and deleting the strongest frequencies from the sample until the strongest frequency is lower than the threshold. Once this happens, the sample is completely sorted, and the sample's index is stored in bins for each note that was contained within the sample. It is important to note that on these subsequent loops, a frequency will only be added to a bin if the same sample's index does not already appear in that bin. This prevents a sample from being sorted twice into the same note's bin.\n",
    "\n",
    "For example, after completely sorting a sample of a C Major chord in the fourth octave, where all three notes were above threshold and there was no noise above the threshold, the notes_db structure would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"A\": [[],[],[],[],[],[]],\n",
    "\"A#\": [[],[],[],[],[],[]],\n",
    "\"B\": [[],[],[],[],[],[]],\n",
    "\"C\": [[],[],[],[sample_index],[],[]],\n",
    "\"C#\": [[],[],[],[],[],[]],\n",
    "\"D\": [[],[],[],[],[],[]],\n",
    "\"D#\": [[],[],[],[],[],[]],\n",
    "\"E\": [[],[],[],[sample_index],[],[]],\n",
    "\"F\": [[],[],[],[],[],[]],\n",
    "\"F#\": [[],[],[],[],[],[]],\n",
    "\"G\": [[],[],[],[sample_index],[],[]],\n",
    "\"G#\":[[],[],[],[],[],[]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the samples from a given input wave are sorted in this manner, it is time to assemble them into the music the user provided. A sample of the notation used to specify the output music is shown below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "E5\n",
    "D#5\n",
    "#\n",
    "E5\n",
    "D#5\n",
    "E5\n",
    "B4\n",
    "D5\n",
    "C5\n",
    "#\n",
    "A4\n",
    "%\n",
    "%\n",
    "C4\n",
    "E4\n",
    "A4\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example consists of the first few measures of Fur Elise. The notes to be played at a given time appear on each line. \"#\" symbols are used to denote boundaries between measures for convenience of reading and writing, and are ignored by the program. \"%\" symbols denote rests, or periods where no notes are played. Each note or rest has the length of one sample. Chords can be denoted by having multiple notes on the same line, separated by commas.\n",
    "\n",
    "First an empty array is initialized to hold the output wave. Each line of the input music file is isolated and split by comma characters. If the line consists of a \"#\" symbol, the line is ignored and the next line is processed. If the line consists of a \"%\" symbol, a sample-length set of zeros is generated and appended to the output wave. Otherwise, for each note symbol on the line, a random sample index is pulled from the note's corresponding bin. The sample whose index was chosen is appended onto the output wave.\n",
    "\n",
    "Once all the lines have been processed, the output wave is packed and saved under a user-provided filename, using functions from waveIO.\n",
    "\n",
    "\n",
    "When I transitioned from my naive approach to a better representation of the STFT, I defined two constants, `WINDOW_LENGTH` and `OVERLAP_PERCENT`. These replaced the tempo, bpm, and dt calculations my naive version had done. `WINDOW_LENGTH` was the length, in data points of the STFT sampling window. `OVERLAP_PERCENT` is how much overlap there will be between consecutive windows. A value ov 0.5 would mean the second window would start at the midpoint of the first, and so on. I also created two functions, `window_rectangular` and `window_hanning`, which apply their given window functions to a `WINDOW_LENGTH` wave segment.\n",
    "\n",
    "Rather than cutting the wave up into equal size segments, the wave is iterated over, with a step size of `WINDOW_LENGTH*(1-OVERLAP_PERCENT)`. In each step, a `WINDOW_LENGTH` sized portion is selected, and the `WINDOW_FUNC` is called upon that segment. Once this is done, the segment is added to an array of samples from the input wave, just as it had been in the naive version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Physical Description\n",
    "\n",
    "In the realm of soundwave analysis, there are a number of \"perspectives\" from which a wave can be viewed. The first of these, and likely the most familiar to many people, is the time domain perspective. The time domain perspective for a given soundwave shows the wave's amplitude over time. In music production, this perspective is often used to visually identify buildups, musical climaxes, and changes from one section of a song to another. It is this perspective which is typically used when a visual representation of a sound wave is needed.\n",
    "\n",
    "<img src=\"Results/figures/vocal_scales_time_domain.png\">[Fig 1]\n",
    "\n",
    "The figure above is an example of the time perspective. From this perspective, it is easily seen that there are four distinct sections, each separated by short segments of much lower volume. However, little information about the contents of those louder sections can be gathered from this view.\n",
    "\n",
    "One aspect of the wave which this view gives no information on, but which is very important for musical application is frequency. The frequency of a wave determines the pitch of the sound a wave makes. The higher the wave's frequency, the higher pitched its note will sound. Musical notes are organized into sets called octaves. An octave consists of twelve different frequencies, called semitones, that represent musical notes. The lowest note in an octave is C, the highest is B. The standard notation \"C4\" stands for the C note in the fourth octave, which has a frequency of 261.63 Hz. To find the frequency of a note *n* semitones higher than frequency *f*, the following formula is used:\n",
    "\n",
    "<b><i>f_new = f \\* 2<sup>n/12</sup></i></b>\n",
    "\n",
    "From this formula, the conclusion can be drawn that a note exactly one octave, or twelve semitones, above frequency *f* has the frequency *2f*:\n",
    "\n",
    "<b><i>f_octave = f \\* 2<sup>12/12</sup> = f \\* 2 = 2f</i></b>\n",
    "\n",
    "With this knowledge, the next logical perspective to look at is the frequency perspective. This perspective is used to determine what frequencies are present in a given wave. To obtain this perspective, the Fourier Transform is performed on the wave. With the resulting amplitudes normalized by dividing them by the number of samples in the wave, the result looks like this:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_ps.png\">[Fig 2]\n",
    "\n",
    "This example shows the frequency perspective of a simple, computer generated wave. Clearly there are three main frequencies present. They are at approximately 440 Hz, 550 Hz, and 660 Hz. The wave used to generate this spectrum consisted of short segments where each of those frequencies (corresponding to A4, C#5, and E5, respectively, the three notes in an A Major chord) is played individually, then a final segment where the three were played together. A second sample wave was generated in which the same notes were played in reverse order, before again finishing with all three played together. Observe the frequency domain of this sample:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_ps_reverse.png\">[Fig 3]\n",
    "\n",
    "The two plots look identical! This illustrates an important aspect of the frequency domain: just as the time domain conveyed no usable information about the frequencies contained within its wave, the frequency domain conveys no information about the times at which its frequencies occur.\n",
    "\n",
    "Ideally, there would be a perspective that can show both frequency and time simultaneously. This way, we could isolate exactly what notes are happening and when. Such a perspective would then allow for hand picking sections of the wave to take samples from, if manual sampling were the goal. As it happens, there is a perspective that does exactly this: the spectrogram. A spectrogram is a plot of frequency strength over time. The horizontal axis typically represents time, while the vertical represents various frequencies. to denote amplitude of a frequency at a given time, the corresponding point on the plot is colored. In the following examples, dark red indicates high amplitude at the frequency and time, and yellow indicates low amplitude. The following are spectrograms provided by matplotlib for the two sample waves used for the power spectra above:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_specgram.png\">[Fig 4]\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_specgram_reverse.png\">[Fig 5]\n",
    "\n",
    "In these spectrograms, the red bands center around the frequencies being played at a given time in the wave. It is clear, looking at them, that the two samples contain the same notes, being played in a different order, and it is clear exactly what that order is for each sample.\n",
    "\n",
    "The spectrograms above were generated easily from wave data using matplotlib's `specgram` function, however the method by which the function generates them is the key to this project. `Specgram` makes use of what is known as the **Short Time Fourier Transform**, or STFT. The STFT is defined as follows:\n",
    "\n",
    "<img src=\"STFT equation.png\">[Fig 6, Bebis]\n",
    "\n",
    "STFT essentially takes the Fourier Transform of very small parts of the wave at a time. This is done by multiplying the wave by a windowing function, which is nonzero only in a small time range centered around time `t'`, and then taking the Fourier Transform of the result. These small transforms are taken over the entire length of the wave by incrementally increasing `t'`, and a matrix can be created showing the intensity of all frequencies in all the small parts of the wave. These matrices can be plotted as the spectrograms seen above.\n",
    "\n",
    "Looking again at the spectrograms, there is one key difference other than simply the order in which the notes are presented. The thicknesses of the frequency bars are different in the two figures. This is a factor of how large the STFT window is. A longer STFT window provides greater frequency precision, showing more clearly exactly what frequencies are represented. There is a tradeoff here, though. Imagine an STFT whose window width was equal to the length of the entire wave. This would be the same as a normal Fourier Transform! That is to say we would lose all indication of the frequencies' timing! The frequency resolution given by STFT with a given window length L is defined by:\n",
    "\n",
    "$\\Delta F = F_{s} / L Hz$\n",
    "\n",
    "Conversely, imagine an STFT with an infinitely short windowing function such as the Dirac Delta Function:\n",
    "\n",
    "<img src=\"STFT infinite short.png\">[Fig 7, Bebis]\n",
    "\n",
    "As you can see, because this windowing function gives a result of zero for all points other than `t'`, the result of the STFT would just be a time spectrum of the wave, losing all frequency representation (Bebis).\n",
    "\n",
    "\n",
    "For the purposes of this project, I am not trying to build a spectrogram, however, but rather take apart the wave into small pieces (samples), which can then be put together in any order to create a new piece of choesive music. The principle, however, remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "I began by building a naive version of the STFT (`sampler-naive.py`). This chopped up the input waves into equally sized chunks (in this case 8939 data points long), and then computed the Fourier Transform on each chunk to give myself a set of equally sized samples with which to build my output wave. This is equivalent to performing an STFT with window size 8939, and $\\Delta_{t'} = 4469$. What this means is that there is no overlap between the STFT windows; the next window begins where the previous window ended. I then created a wave file which consisted of a chromatic scale from C4 to G#6. This wave was called `bigsample.wav`, Every note in that range was represented, which made it ideal for sampling and putting together into a new piece.\n",
    "\n",
    "I ran `bigsample.wav` through my naive function, which chopped it up successfully, sorted the samples into note bins (a threshold of 1400 was used) and rearranged the samples into `furelise-naive.wav`. I also plotted how many samples per note between C4 and G#6 the function was able to get from the wave. That plot is below:\n",
    "\n",
    "<img src=\"Results/figures/note_numbers_naive.png\">[Fig 8]\n",
    "\n",
    "There are at least two samples per note, with some notes having three. The reason some notes have an extra sample has to do with sample windows which lie on boundaries between two notes playing. These boundaries would count as a sample for both of the two notes contained within the window. Listening to `furelise-naive.wav` confirms this. Every so often a sample is heard that sounds like two notes, each a fraction of the normal note length. These are those boundary-case samples. Comparing the spectrogram of `furelise-naive.wav` to that of an analytically generated `analytic_furelise.wav` files, we can see these boundary cases causing some discrepancies, namely around the 2.25 second mark: \n",
    "\n",
    "<img src = \"Results/figures/analytic_furelise_specgram.png\">[Fig 9]\n",
    "\n",
    "<img src = \"Results/figures/naive_furelise_specgram.png\">[Fig 10]\n",
    "\n",
    "With a better understanding of STFT, I restructured my naive code to make manipulating the parameters of the STFT more manageable. The result was `sampler-stft.py`. To ensure that it was working as desired, I set it up with the same initial conditions as my naive approach had used: window size set to 8939, window step set to the window size, and using the same `bigsample.wav` as the input wave. The plot of samples per note is identical to that generated by my naive model:\n",
    "\n",
    "<img src=\"Results/figures/note_numbers_stft_noverlap.png\">[Fig 11]\n",
    "\n",
    "To reduce the boundary cases that make it into the output wave, it makes sense to shorten the window length. While the number of boundary cases is likely to stay the same or even increase slightly (There are only so many note boundaries in the wave to begin with), the number ofsamples pulled from the file will increase more so that the boundary cases are less likely to be selected. Using a  window length of 4500, the samples per note plot changes in a couple notable ways:\n",
    "\n",
    "<img src=\"Results/figures/note_numbers_smallwindow.png\">[Fig 12]\n",
    "\n",
    "As expected the number of samples has roughly doubled. Most notes now have 4 samples instead of two, with some notes having five. More importantly, though, C4, C#4, and E4 have no samples! There are two factors at play here: frequency resolution, and tolerance in my note sorting algorithm. By reducing my window size, I have also reduced my frequency resolution from approximately 4.9 Hz to 9.8 Hz. With this lower frequency resolution, I need to raise my tolerance to be less restrictive when sorting notes. Raising this tolerance from 1% to 2% gives the following plot as a result:\n",
    "\n",
    "<img src=\"Results/figures/note_numbers_smallwindow_higher_tolerance.png\">[Fig 13]\n",
    "\n",
    "The notes that were missing from the prior plot are now present again. Arranging these samples into the Fur Elise email resulted in `furelise-stft-doubletol.wav`. Listening to this sample provides litle information on whether the boundary conditions are rarer, however, because the shorter window led to shorter samples. With samples this short, it is hard to differentiate full-note samples from samples that sound shorter due to being comprised of two notes separated by a note boundary. What makes it difficult is the fact that there is an audible \"click\" sound when one sample stops and another begins. This is because there is an amplitude discontinuity at the point where the two samples meet.\n",
    "\n",
    "To reduce this issue, we use a different windowing function. Rather than a simple rectangular function, we use a Hanning function, which is a modified cosine function. The Hanning function looks like this:\n",
    "\n",
    "<img src=\"hanning.png\">[Fig 14: from www.diracdelta.co.uk]\n",
    "\n",
    "Using the Hanning function as the windowing function ensures that the beginning and end of all samples will be zero, and will eliminate the harsh amplitude jumps that can happen when two samples are out of phase with one another. Since this reduces the amplitude of all frequencies except those in the very middle of the STFT window, this has the side effect of reducing the magnitude of each frequency returned by the Fourier Transform. For this reason, threshold values that may have worked for the rectangular window function can be too high, resulting in no samples sorted at all. Using this hanning function on `bigsample.wav` with window length of 4500 samples, 2% tolerance, and varying threshold values revealed that 1067 was the maximum threshold that could be used which would return samples for all notes in the sample, while 895 was the minumum threshold that would return as many samples as were returned by the rectangular window function with threshold 1400 and the other initial conditions the same.\n",
    "\n",
    "The following is a spectrogram of a Fur Elise melody generated from `bigsample.wav` using the Hanning window function with threshold 895 and window size 8939 (`furelise_hanning_8939_895.wav`):\n",
    "\n",
    "<img src=\"Results/figures/furelise_hanning_8939_specgram.png\">[Fig 15]\n",
    "\n",
    "Comparing to Figures 9 and 10, the most noticeable difference is the lack of vertical red bars between each sample. Those vertical red bars are the \"clicks\" that were heard at the boundary of the samples. The Hanning window function has served its purpose! Notice that there are still a couple of those vertical bars. Listening to `furelise_hanning_8939_895.wav` confirms that there are still a couple of audible clicks. These, however, are simply artifacts of the source `bigsample.wav` wave. This source did not fade notes into one another, but rather abruptly cut them off, creating the same kinds of amplitude discontinuities that were previously seen in the rectangular windowed STFT spectrograms. To confirm this, I created a new version of `bigsample.wav` called `bigsample_hanning.wav`. In this file, I used the same hanning function on each note to smooth the boundaries between them. Running the sampler (with the most recently mentioned conditions) on `bigsample_hanning.wav` gave the following spectrogram:\n",
    "\n",
    "<img src=\"Results/figures/furelise_hanning_samplehanning_specgram.png\">[Fig 16]\n",
    "\n",
    "As expected, the \"clicks\" are gone! This is confirmed by listening to `furelise_hanning_8939_895_samplehanning.wav`.\n",
    "\n",
    "\n",
    "These results are what I was hoping for, but they came from very simple, computer-generated audio. This audio has no noise or harmonics the way that real-world audio would. The next step was to test my sampler with recordings of real instruments and voices. I found a recording of a three octave chromatic scale played on the piano to start with (`piano_three_octaves.wav`). The spectrogram of this input wave looks like this:\n",
    "\n",
    "<img src=\"Results/figures/piano_3_octaves_specgram.png\">[Fig 17]\n",
    "\n",
    "From this spectrogram, you can clearly see the gradually rising pitch of the notes being played, but you can also see weaker frequencies appearing above each root note. Listening to the sample suggests only one note is played at a time. Those higher frequencies are **harmonics**. The piano, along with most other instruments who produce their sound via a vibrating cord produces frequencies called harmonics with every note played. These harmonc frequencies occur at multiples of the fundamental frequency (also called the first harmonic). For example, the second harmonic of a 440 Hz fundamental frequency would be 880 Hz. These harmonics have interesting effects on the output wave the sampler generates. Starting with the naive sampler with note length 8939 data points, amplitude threshold 500, and frequency tolerance at 1%, the following spectrogram is generatedby the resulting Fur Elise melody, with samples taken from `piano_three_octaves.wav`:\n",
    "\n",
    "<img src=\"Results/figures/furelise_naive_piano.png\">[Fig 18]\n",
    "\n",
    "Figure 18 looks almost nothing like the spectrograms of the same melody shown in Figures 15 and 16! Listening to the `furelise_naive_piano.wav` wave file also sounds very much unlike the melodies made of computer generated sounds. The notes, for the most part, sound correct, however they are played in the wrong octave at times, leading to a much different sound. The reason for this is because of those harmonics! Take a look at the first note in Figure 16 and compare it to the first note in Figure 18. This first note is supposed to be an E5, which has a frequency of approximately 659 Hz. In Figure 16, this area is clearly shown as being the strongest frequency of the first note. In Figure 18, however, that frequency range, while certainly strong, is not nearly the strongest frequency. It is merely a harmonic of the lower octave E4 note. The same is true for many of the other notes that sound \"off\" in the melody. They are the same notes, at different octaves.\n",
    "\n",
    "The same thing happens, for the same reasons, when we apply the Hanning-Windowed STFT to `piano_three_octaves.wav` (with the threshold lowered to 310). As in the computer-generated realm, using the Hanning function rids the final output (`furelise_piano_hanning.wav`) of the clicking sounds between samples (See spectrogram in Figure 19). Listening to the output wave, however, it is hard to even discern that the source instrument is a piano. The short, uniform length segments with equal attack and decay are incredibly unlike the instrument's normal sounds, which typically emplys high sustain and a significantly longer decay than attack.\n",
    "\n",
    "<img src=\"Results/figures/furelise_piano_hanning_specgram.png\">[Fig 19]\n",
    "\n",
    "As one final attempt to get a \"natural\" sounding piano wave, I again tweaked my appraoch to STFT. I set the `OVERLAP_PERCENT` to 0.5, meaning samples pulled from the input wave would overap with half of their preceding sample. Then I also made slight changes to the way the output wave is built. Since the Hanning window function is a modified cosine, I also had my output samples overlap one another. In other words, the first half of the second sample was added to the second half of the first sample, and so on. By balancing the attack of the second sample with the decay of the first sample, I was able to eliminate the unnatural silences between notes in `furelise_piano_hanning.wav`. This, more natural output wave was called `furelise_piano_hanning_overlap`. Its spectrogram can be seen in Figure 20.\n",
    "\n",
    "<img src=\"Results/figures/furelise_piano_hanning_overlap_specgram.png\">[Fig 20\n",
    "\n",
    "Finally, I ran through the same set of experiments using a recording of female vocals. The recording I used as input here (`vocalscales.wav`) provides a diverse set of frequencies in the same range as those I have been working with, however it misses a couple of the lower notes in the 4th octave. Rather than fill that gap in with computer generated samples, I simply changed the notes in my music file (`furelise.txt`) so that the C4 notes, which are not present in the input wave, are replaced with C5 notes. Figures 19 and 20 show the spectrograms generated by the naive and the hanning STFT samplers, respectively. Both used window sizes of 8939 data points. The naive sampler used a threshold of 1000, the Hanning sampler used a values of 500. Both used tolerances of 1%.\n",
    "\n",
    "\n",
    "<img src=\"Results/figures/furelise_naive_vocals_specgram.png\">[Fig 21]\n",
    "\n",
    "<img src=\"Results/figures/furelise_hanning_vocal_specgram.png\">[Fig 22]\n",
    "\n",
    "Listening to the output wave (`furelise_vocal_hanning.wav`), parts of the melody sound correct, others sound very incorrect. Looking at the spectrogram makes it clear why this is: some of the notes are changing frequency drastically during the note's length! The first note, for example, increases in frequency drastically! This results in a sliding sound, which is clearly not what is needed for a musical sampling application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "**Computer Generated input samples:**\n",
    "The results of my experiments using computer generated samples were more of a baseline to familiarize myself with the process than anything else. The results that I got were consistent with what I expected. With total control over the structure of the input waves (what frequencies were represented, how many were represented at once, etc), as well as the lack of noise and harmonics inherent to pure sinusoidal data, I was able to construct input waves to serve my purposes perfectly. `bigsample.wav` is a perfect example of this. I needed a long sample that contained every note in a relatively large frequency range, so I just build a chromatic scale sample.\n",
    "\n",
    "**Piano input sample:**\n",
    "I knew going into this project that transitioning from computer generated input waves to real-world instrumental recordings would be a pretty large hurdle. Dealing with the harmonics generated by the piano was surprisingly difficult. The lower notes had such strong harmonic frequencies that they would often get sorted into not only their note bins, but also the note bins of their harmonics. This is what led to the output waves having low notes where they should not have been. I attempted to account for this by setting the threshold relative to the strongest frequency in a sample. My idea was that, in the low notes, the harmonics would fall under some ratio and not be counted in the note's sorting. For example, I would start with a base threshold of 310, but once I found the max amplitude in a sample, I would set the threshold to the maximum of 310, and a fraction of that amplitude. When the threshold was half the max amplitude, the higher notes were simply no longer represented at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "Bebis. Short Time Fourier Transform (STFT). Retrieved from University of Nevada, Reno Computer Science and Engineering department, via Google: www.cse.unr.edu%2F~bebis%2FCS474%2FLectures%2FShortTimeFourierTransform.ppt\n",
    "\n",
    "Heinzel, Rüdiger, Schilling. \"Spectrum and Spectral density estimation by the Discrete Fourier Transform (DFT), including a comprehensive list of window functions and some new flat-top windows.\" February 15, 2002. Max-Planck-Institute für Gravitationsphysik. December 8, 2015 <https://holometer.fnal.gov/GH_FFT.pdf>\n",
    "\n",
    "\"Piano Key Frequencies.\" Wikipedia. December 8, 2015. <https://en.wikipedia.org/wiki/Piano_key_frequencies>\n",
    "\n",
    "Free Engineering Lectures. \"The Short Time Fourier Transform | Digital Signal Processing.\" 2014. Web. 8 Dec. 2015. <https://www.youtube.com/watch?v=g1_wcbGUcDY>\n",
    "\n",
    "\"Piano Keys C3-C6\". Posted on www.freesounds.com by user Tesabob2001 on October 21, 2013. <https://www.freesound.org/people/Tesabob2001/sounds/203494/>.\n",
    "\n",
    "\"tmp_Vocal Scales1729437025.mp3\". Posted on www.freesounds.com by user tweedledee3 on January 29, 2014. <https://www.freesound.org/people/tweedledee3/sounds/216504/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
