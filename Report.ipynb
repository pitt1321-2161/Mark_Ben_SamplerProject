{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Inspired by the wave analyses and manipulations that homework 9 required, as well as by our own experiences as musicians, we chose to create what we are calling an autosampler. In a musical context, sampling is the act of selecting and isolating small parts (known as samples) of a recording, and using those samples as part of a new piece of music. Sampling has always been common in the world of hip hop, where drum beat samples are looped and used as a backing track. More recently, some producers of electronic dance music have risen to prominence whose music consists almost entirely of short samples, mixed together and played as their own instrument. In either case, the producer needed to manually create his own samples. Using modern-day software, this is done by simply selecting a starting and an ending point on a recording's sound wave, and taking the part of the wave that lies between those two points. While this process sounds simple enough, it becomes quite tedious when you have large numbers of recordings to analyze, listen to for potential samples, and cut samples from. Our goal in this project is to automatically cut samples from user-provided wave files, and arrange those samples to produce output in the tune of a user-provided melody. For the examples presented in this paper, we have used the melody of Beethoven's well known piece Fur Elise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE DESCRIPTION:\n",
    "\n",
    "The meat of this project is the functionality of chopping up an input wave into samples, and then properly categorizing and storing the resulting pieces. In our initial, naive approach, we decided to chop the waves such that one sample would have the length of an eighth note, given a predefined tempo in beats per minute. We arbitrarily chose a tempo of 148 beats per minute. This translated to approximately 2.467 beats per second, or 0.2027 seconds per note since the eighth note is traditionally half a beat. Using a standard sampling rate of 44.1 kHz, this meant that each note would be 8939 data points long.\n",
    "\n",
    "With this information, the next step is to iterate over each input wave provided by the user. Using the functions from waveIO.py, each file is read in and unpacked.The wave is then split, as mentioned above, into pieces 8939 data points long. This list of points serves as the samples that will be analyzed, sorted, and built into a final output wave.\n",
    "\n",
    "This list of samples is then iterated over for sorting and storage. Samples shorter than 8939 samples long are discarded, which prevents shortened samples such as those that can occur at the end of a wave from being sorted with the samples of uniform length. The Fourier Transform of the sample is taken using Numpy's fft module, and then divided by the length of the sample in data points to normalize its amplitudes. Numpy'sfftfreq function is used to create the frequency bins for the sample, and the sample's strongest frequency is found.\n",
    "\n",
    "This is where the amplitude normalization becomes important. The amplitude of the strongest frequency is compared to a predefined threshold value. If the amplitude is lower than this threshold, then no frequencies are extracted from the sample. If the amplitude is greater than the threshold, the sample's data is added to a large list of samples, and its index within that list is noted. The sample proceeds with sorting. In that case, the frequency is compared with a dictionary containing baseline frequencies for the twelve semitones in a musical octave. Since the A4 (this notation stands for the note A in the fourth octave, with A being the tenth not in the octave) note with an ideal frequency of 440.0 Hz is typically used as a standard, I used the frequencies of all the fourth octave notes to populate this dictionary.\n",
    "\n",
    "If the sample's frequency is lower than the lowest frequency in the dictionary (with about 1% tolerance), then it must be part of a lower octave. Likewise, if it is higher than the highest frequency in the dictionary (with the same tolerance), it must be part of a higher octave. In the former case, I divide a multiplier value (initially set to 1) by 2 and compare the sample frequency to the highest and lowest frequencies in the dictionary, multiplied by the multiplier value. If the frequency is still too low, I again halve the multiplier, continuing this process until the sample frequency falls between the highest and lowest dictionary frequencies times the multiplier. Alternatively, in the case that the sample frequency was too high, I perform the same process, but double the multiplier instead of halving it on each unsuccessful comparison.\n",
    "\n",
    "Having ascertained the octave of the sample frequency, and with specific values of the multiplier corresponding to specific octaves, the sample can be properly stored. The dictionary of note frequencies is iterated over, with each note's frequency value multiplied by the multiplier from earlier. If the sample frequency is within 1% of this multiplied dictionary value, its index in the larger sample list is stored in a list within a dictionary of the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notes_db = {\n",
    "\"A\": [[],[],[],[],[],[]],\n",
    "\"A#\": [[],[],[],[],[],[]],\n",
    "\"B\": [[],[],[],[],[],[]],\n",
    "\"C\": [[],[],[],[],[],[]],\n",
    "\"C#\": [[],[],[],[],[],[]],\n",
    "\"D\": [[],[],[],[],[],[]],\n",
    "\"D#\": [[],[],[],[],[],[]],\n",
    "\"E\": [[],[],[],[],[],[]],\n",
    "\"F\": [[],[],[],[],[],[]],\n",
    "\"F#\": [[],[],[],[],[],[]],\n",
    "\"G\": [[],[],[],[],[],[]],\n",
    "\"G#\":[[],[],[],[],[],[]]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where each note has a list of sub-lists, and each sub-list holds the index values of samples that contain the corresponding note in a specific octave.\n",
    "\n",
    "If the code were to stop at this point, it could deal perfectly fine with simple input waves of single notes. Chords, however, would pose a problem, as only one frequency is pulled from each sample. To solve this problem, after the sample is sorted into one of the bins above, the strongest frequency is deleted from the Fourier Transform data, as is the frequency bin that corresponded to that frequency. The octave multiplier is reset to 1, and the next strongest frequency is pulled from the Fourier data. Its amplitude is once again compared to the threshold, and the whole process repeats itself, selecting, sorting, and deleting the strongest frequencies from the sample until the strongest frequency is lower than the threshold. Once this happens, the sample is completely sorted, and the sample's index is stored in bins for each note that was contained within the sample. It is important to note that on these subsequent loops, a frequency will only be added to a bin if the same sample's index does not already appear in that bin. This prevents a sample from being sorted twice into the same note's bin.\n",
    "\n",
    "For example, after completely sorting a sample of a C Major chord in the fourth octave, where all three notes were above threshold and there was no noise above the threshold, the notes_db structure would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"A\": [[],[],[],[],[],[]],\n",
    "\"A#\": [[],[],[],[],[],[]],\n",
    "\"B\": [[],[],[],[],[],[]],\n",
    "\"C\": [[],[],[],[sample_index],[],[]],\n",
    "\"C#\": [[],[],[],[],[],[]],\n",
    "\"D\": [[],[],[],[],[],[]],\n",
    "\"D#\": [[],[],[],[],[],[]],\n",
    "\"E\": [[],[],[],[sample_index],[],[]],\n",
    "\"F\": [[],[],[],[],[],[]],\n",
    "\"F#\": [[],[],[],[],[],[]],\n",
    "\"G\": [[],[],[],[sample_index],[],[]],\n",
    "\"G#\":[[],[],[],[],[],[]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the samples from a given input wave are sorted in this manner, it is time to assemble them into the music the user provided. A sample of the notation used to specify the output music is shown below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "E5\n",
    "D#5\n",
    "#\n",
    "E5\n",
    "D#5\n",
    "E5\n",
    "B4\n",
    "D5\n",
    "C5\n",
    "#\n",
    "A4\n",
    "%\n",
    "%\n",
    "C4\n",
    "E4\n",
    "A4\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example consists of the first few measures of Fur Elise. The notes to be played at a given time appear on each line. \"#\" symbols are used to denote boundaries between measures for convenience of reading and writing, and are ignored by the program. \"%\" symbols denote rests, or periods where no notes are played. Each note or rest has the length of one sample. Chords can be denoted by having multiple notes on the same line, separated by commas.\n",
    "\n",
    "First an empty array is initialized to hold the output wave. Each line of the input music file is isolated and split by comma characters. If the line consists of a \"#\" symbol, the line is ignored and the next line is processed. If the line consists of a \"%\" symbol, a sample-length set of zeros is generated and appended to the output wave. Otherwise, for each note symbol on the line, a random sample index is pulled from the note's corresponding bin. The sample whose index was chosen is appended onto the output wave.\n",
    "\n",
    "Once all the lines have been processed, the output wave is packed and saved under a user-provided filename, using functions from waveIO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Physical Description\n",
    "\n",
    "In the realm of soundwave analysis, there are a number of \"perspectives\" from which a wave can be viewed. The first of these, and likely the most familiar to many people, is the time domain perspective. The time domain perspective for a given soundwave shows the wave's amplitude over time. In music production, this perspective is often used to visually identify buildups, musical climaxes, and changes from one section of a song to another. It is this perspective which is typically used when a visual representation of a sound wave is needed.\n",
    "\n",
    "<img src=\"Results/figures/vocal_scales_time_domain.png\">\n",
    "\n",
    "The figure above is an example of the time perspective. From this perspective, it is easily seen that there are four distinct sections, each separated by short segments of much lower volume. However, little information about the contents of those louder sections can be gathered from this view.\n",
    "\n",
    "One aspect of the wave which this view gives no information on, but which is very important for musical application is frequency. The frequency of a wave determines the pitch of the sound a wave makes. The higher the wave's frequency, the higher pitched its note will sound. Musical notes are organized into sets called octaves. An octave consists of twelve different frequencies, called semitones, that represent musical notes. The lowest note in an octave is C, the highest is B. The standard notation \"C4\" stands for the C note in the fourth octave, which has a frequency of 261.63 Hz. To find the frequency of a note *n* semitones higher than frequency *f*, the following formula is used:\n",
    "\n",
    "<i>f_new = f \\* 2<sup>n/12</sup></i>\n",
    "\n",
    "From this formula, the conclusion can be drawn that a note exactly one octave, or twelve semitones, above frequency *f* has the frequency *2f*:\n",
    "\n",
    "<i>f_octave = f \\* 2<sup>12/12</sup> = f \\* 2 = 2f</i>\n",
    "\n",
    "With this knowledge, the next logical perspective to look at is the frequency perspective. This perspective is used to determine what frequencies are present in a given wave. To obtain this perspective, the Fourier Transform is performed on the wave. With the resulting amplitudes normalized by dividing them by the number of samples in the wave, the result looks like this:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_ps.png\">\n",
    "\n",
    "This example shows the frequency perspective of a simple, computer generated wave. Clearly there are three main frequencies present. They are at approximately 440 Hz, 550 Hz, and 660 Hz. The wave used to generate this spectrum consisted of short segments where each of those frequencies (corresponding to A4, C#5, and E5, respectively, the three notes in an A Major chord) is played individually, then a final segment where the three were played together. A second sample wave was generated in which the same notes were played in reverse order, before again finishing with all three played together. Observe the frequency domain of this sample:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_ps_reverse.png\">\n",
    "\n",
    "The two plots look identical! This illustrates an important aspect of the frequency domain: just as the time domain conveyed no usable information about the frequencies contained within its wave, the frequency domain conveys no information about the times at which its frequencies occur.\n",
    "\n",
    "Ideally, there would be a perspective that can show both frequency and time simultaneously. This way, we could isolate exactly what notes are happening and when. Such a perspective would then allow for hand picking sections of the wave to take samples from, if manual sampling were the goal. As it happens, there is a perspective that does exactly this: the spectrogram. A spectrogram is a plot of frequency strength over time. The horizontal axis typically represents time, while the vertical represents various frequencies. to denote amplitude of a frequency at a given time, the corresponding point on the plot is colored. In the following examples, dark red indicates high amplitude at the frequency and time, and yellow indicates low amplitude. The following are spectrograms provided by matplotlib for the two sample waves used for the power spectra above:\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_specgram.png\">\n",
    "\n",
    "<img src=\"Results/figures/sample_wave_specgram_reverse.png\">\n",
    "\n",
    "In these spectrograms, the red bands center around the frequencies being played at a given time in the wave. It is clear, looking at them, that the two samples contain the same notes, being played in a different order, and it is clear exactly what that order is for each sample.\n",
    "\n",
    "The spectrograms above were generated easily from wave data using matplotlib's `specgram` function, however the method by which the function generates them is the key to this project. `Specgram` makes use of what is known as the **Short Time Fourier Transform**, or STFT. STFT essentially takes the Fourier Transform of very small parts of the wave at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
